[
  {
    "objectID": "teaching/Simple_Linear_Regression.html",
    "href": "teaching/Simple_Linear_Regression.html",
    "title": "Sangsuk Yoon",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\n#@title Figure Settings\nimport ipywidgets as widgets       # interactive display\n%config InlineBackend.figure_format = 'retina'\nplt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")\n\n\n#@title Helper functions\n\ndef plot_observed_vs_predicted(x, y, y_hat, theta_hat, intercept_hat):\n  \"\"\" Plot observed vs predicted data\n\n  Args:\n      x (ndarray): observed x values\n  y (ndarray): observed y values\n  y_hat (ndarray): predicted y values\n\n  \"\"\"\n  fig, ax = plt.subplots()\n  ax.scatter(x, y, label='Observed')  # our data scatter plot\n  ax.plot(x, y_hat, color='r', label='Fit')  # our estimated model\n  # plot residuals\n  ymin = np.minimum(y, y_hat)\n  ymax = np.maximum(y, y_hat)\n  ax.vlines(x, ymin, ymax, 'g', alpha=0.5, label='Residuals')\n  ax.set(\n      title=fr\"Intercept = {intercept_hat}, Slope = {theta_hat:0.2f}, MSE = {mse(x, y, theta_hat, intercept_hat):.2f}\",\n      xlabel='customer satisfaction',\n      ylabel='sales ($)'\n  )\n  ax.legend()\n\n\ndef mse(x, y, theta_hat, intercept):\n  \"\"\"Compute the mean squared error\n\n  Args:\n    x (ndarray): An array of shape (samples,) that contains the input values.\n    y (ndarray): An array of shape (samples,) that contains the corresponding\n      measurement values to the inputs.\n    theta_hat (float): An estimate of the slope parameter\n    \n  Returns:\n    float: The mean squared error of the data with the estimated parameter.\n  \"\"\"\n  ####################################################\n  ## TODO for students: compute the mean squared error\n  # Fill out function and remove\n  # raise NotImplementedError(\"Student exercise: compute the mean squared error\")\n  ####################################################\n\n  # Compute the estimated y \n  y_hat = intercept + theta_hat * x\n\n  # Compute mean squared error\n  # mse = 1/len(x) * np.sum((y-y_hat)**2)\n  mse = np.mean((y-y_hat)**2)\n\n  return mse\n\n\n# @title \n\n# @markdown Execute this cell to generate some simulated data\n\n# setting a fixed seed to our random number generator ensures we will always\n# get the same psuedorandom number sequence\nnp.random.seed(121)\n\n# Let's set some parameters\ntheta = 1.2\nn_samples = 30\nconstant = 3\n\n# Draw x and then calculate y\nx = 6 * np.random.rand(n_samples) + 1  # sample from a uniform distribution over [0,10)\nnoise = np.random.randn(n_samples)  # sample from a standard normal distribution\ny = constant + theta * x + noise\n\n# Plot the results\nfig, ax = plt.subplots()\nax.scatter(x, y)  # produces a scatter plot\nax.set(xlabel='customer satisfaction', ylabel='sales ($)');\n\n\n\n\n\n\n\n\n\n#@title\n\n#@markdown Execute this cell to visualize estimated models\ntheta_hats = [0.5, 1, 1.5]\nintercepts = [0, 2, 4]\n\nfig, axes = plt.subplots(ncols=3, figsize=(18, 4))\nfor theta_hat, ax, intercept in zip(theta_hats, axes, intercepts):\n\n  # True data\n  ax.scatter(x, y, label='Observed')  # our data scatter plot\n\n  # Compute and plot predictions\n  y_hat = intercept + theta_hat * x\n  ax.plot(x, y_hat, color='r', label='Fit')  # our estimated model\n\n  ax.set(\n      title= fr'Intercept = {intercept}, Slope = {theta_hat}, MSE = {mse(x, y, theta_hat, intercept):.2f}',\n      xlabel='customer satisfaction',\n      ylabel='sales ($)'       \n  );\n\naxes[0].legend()\n\n\n\n\n\n\n\n\n\n#@title \n\n#@markdown Make sure you execute this cell to enable the widget!\n\n@widgets.interact(intercept_hat=widgets.FloatSlider(6.0, min=0.0, max=10.0),\n                  slope_hat=widgets.FloatSlider(0.0, min=-2.0, max=2.0))\n\ndef plot_data_estimate(slope_hat, intercept_hat):\n  y_hat = intercept_hat + slope_hat * x\n  plot_observed_vs_predicted(x, y, y_hat, slope_hat, intercept_hat)\n\n\n\n\n\n#@title Calculate OLS estimators\nX = np.hstack((np.ones((n_samples, 1)), x.reshape(30,1)))\ntheta_hat = np.linalg.inv(X.T@X)@X.T@y\nprint(f'Intercept = {theta_hat[0]:.2f}, Slope = {theta_hat[1]:.2f}, MSE = {mse(x, y, theta_hat[1], theta_hat[0]):.2f}')\n\nIntercept = 2.26, Slope = 1.36, MSE = 1.35"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sangsuk Yoon",
    "section": "",
    "text": "I am an Associate Professor of Marketing at University of Dayton. I’m studying cognitive biases underlying human decision making, and seek to apply findings from laboratory studies in judgment and decision making to consumer behavior (e.g., consumer preference, financial decision making, advertisement, and risk communications). I am also interested in using a multi-methodological approach, including behavioral, computational modeling, eye-tracking and fMRI, to obtain converging evidence about the underlying mechanisms of decision biases.\n\n\nPublications\n\nYoon, S. & Venkatraman, V. “Choosing versus rejecting: The effect of decision mode on subsequent preferential choices” Journal of Behavioral Decision Making (2025) [PDF] | [OSF] | [Journal]\nDugan, R., Edelblum, A., Kalra, A., Lee, N. Y., & Yoon, S. “How social media and flexible work arrangements harden salespeople to abusive supervision” Industrial Marketing Management (2024) [PDF] | [Journal]\nThe Forecasting Collaborative. “Insights into the accuracy of social scientists’ forecasts of societal change” Nature Human Behavior (2023) [PDF] | [Journal]\nChae, B., Yoon, S., Baskin, E., & Zhu. R. “The lasting smell of temptation: Counteractive effects of indulgent food scents” Journal of Business Research (2023) [PDF] | [OSF] | [Journal]\nBago, B., Kovacs, M., …, Yoon, S., …, & Aczel, B. “Situational factors shape moral judgments in the trolley dilemma in Eastern, Southern and Western countries in a culturally diverse sample” Nature Human Behavior (2022) [PDF] | [Journal]\nChoi, M. & Yoon, S. “Asymmetric underlying mechanisms of relation-based and property-based noun-noun conceptual combination” Frontiers in Psychology: Cognition (2021) [PDF] | [OSF] | [Journal]\nVenkatraman, V. & Yoon, S. “Adaptivity in decision-making strategies across age: Process insights and implications” Journal of Marketing Behavior (2020) [PDF] | [Journal]\nBotvinik-Nezer, R., Holzmeister, F., …, Yoon, S., …, & Schonberg, T. “Variability in the analysis of a single neuroimaging dataset by many teams.” Nature (2020) [PDF] | [Journal]\nYoon, S., Fong, N., & Dimoka, A. “The robustness of anchoring effects on preferential judgments.” Judgment and Decision Making (2019) [PDF] | [OSF] | [Journal]\nYoon, S., & Fong, N. “Uninformative anchors have persistent effects on valuation judgments.” Journal of Consumer Psychology (2019) [PDF] | [OSF] | [Journal]\nSilberzahn, R., Uhlmann, E. L., Martin, D. P., Anselmi, P., … Yoon, S., & Nosek, B. A. “Many analysts, one data set: Making transparent how variations in analytic choices affect results.” Advances in Methods and Practices in Psychological Science (2018) [PDF] | [Journal]\nYoon, S., Vo, K., & Venkatraman, V. “Variability in Decision Strategies Across Description‐based and Experience‐based Decision Making.” Journal of Behavioral Decision Making (2017) [PDF] | [Journal]\nChoi, M., Yoon, S., & Oh, J. “Development and Validation of the Korean Illness Perception Questionnaire (K-IPQ).” Health Communication (2016; in Korean) [PDF] | [Journal]\nUehara, T., Li, B., Kim, BM., Yoon, S. S., Quach, QK., Kim, H., & Chon. TS. “Inferring conflicting behavior of zebrafish (Danio rerio) in response to food and predator based on a self-organizing map (SOM) and intermittency test.” Ecological Informatics (2015) [PDF] | [Journal]\nYoon, S., & Shin, H. J. “The Effect of Substitute on WTA and WTP.” The Korean Journal of Cognitive and Biological Psychology (2010; in Korean) [PDF]"
  },
  {
    "objectID": "teaching/Multiple_Linear_Regression.html",
    "href": "teaching/Multiple_Linear_Regression.html",
    "title": "Sangsuk Yoon",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\n#@title Figure Settings\nimport ipywidgets as widgets \n%config InlineBackend.figure_format = 'retina'\nplt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")\n\n\n#@title Helper Functions\n\ndef plot_fitted_polynomials(x, y, theta_hat):\n  \"\"\" Plot polynomials of different orders\n\n  Args:\n    x (ndarray): input vector of shape (n_samples) \n    y (ndarray): vector of measurements of shape (n_samples)\n    theta_hat (dict): polynomial regression weights for different orders\n  \"\"\"\n\n  x_grid = np.linspace(x.min() - .5, x.max() + .5)\n\n  plt.figure()\n\n  for order in range(0, max_order + 1):\n    X_design = make_design_matrix(x_grid, order)\n    plt.plot(x_grid, X_design @ theta_hat[order]);\n\n  plt.ylabel('y')\n  plt.xlabel('x')\n  plt.plot(x, y, 'C0.');\n  plt.legend([f'order {o}' for o in range(max_order + 1)], loc=1)\n  plt.title('polynomial fits')\n  plt.show()\n\n\n#@title\n#@markdown Execute this cell to simulate some data\n\n# Set random seed for reproducibility\nnp.random.seed(1234)\n\n# Set parameters\ntheta = [0, 3, -3]\nn_samples = 40\nconstant = 2\n\n# Draw x and calculate y\nn_regressors = len(theta)\nx0 = np.ones((n_samples, 1)) # Intercept\nx1 = np.random.uniform(1, 7, (n_samples, 1)) #Slope for the first IV (Price)\nx2 = np.random.uniform(0, 10, (n_samples, 1)) # Slope for the second IV (Customer Satisfaction)\nX = np.hstack((x0, x1, x2))\nnoise = np.random.randn(n_samples)\ny = X @ theta + noise + constant\n\n\nax = plt.subplot(projection='3d')\nax.plot(X[:,1], X[:,2], y, '.')\n\nax.set(\n    xlabel='customer satisfaction',\n    ylabel='price',\n    zlabel='sales'\n)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n#@title\n#@markdown Execute this cell to visualize data and predicted plane\n\n@widgets.interact(intercept_hat=widgets.FloatSlider(0.0, min=-5.0, max=5.0),\n                  satisfaction_hat=widgets.FloatSlider(0.0, min=-4.0, max=4.0),\n                  price_hat=widgets.FloatSlider(0.0, min=-4.0, max=4.0))\n\n\ndef interactive_multiple_regression(intercept_hat, satisfaction_hat, price_hat): \n  theta_hat = [intercept_hat, satisfaction_hat, price_hat]\n  xx, yy = np.mgrid[0:10:10j, 0:10:10j]\n  y_hat_grid = np.array([xx.flatten(), yy.flatten()]).T @ theta_hat[1:] + theta_hat[0]\n  y_hat_grid = y_hat_grid.reshape((10, 10))\n\n  ax = plt.subplot(projection='3d')\n  ax.plot(X[:, 1], X[:, 2], y, '.')\n  ax.plot_surface(xx, yy, y_hat_grid, linewidth=0, alpha=0.5, color='C1',\n                  cmap=plt.get_cmap('coolwarm'))\n\n  y_hat = X @ theta_hat\n  \n  for i in range(len(X)):\n    ax.plot((X[i, 1], X[i, 1]),\n            (X[i, 2], X[i, 2]),\n            (y[i], y_hat[i]),\n            'g-', alpha=.5)\n\n  ax.set(\n      title=fr\"Intercept = {intercept_hat}, CS = {satisfaction_hat}, Pr = {price_hat}, MSE = {np.mean((y - y_hat)**2):.2f}\",\n      xlabel='customer satisfaction',\n      ylabel='price',\n      zlabel='sales ($)'\n  )\n  plt.tight_layout()\n\n\n\n\n\n#@title Calculate OLS estimators\n\ndef ordinary_least_squares(X, y):\n  \"\"\"Ordinary least squares estimator for linear regression.\n\n  Args:\n    x (ndarray): design matrix of shape (n_samples, n_regressors)\n    y (ndarray): vector of measurements of shape (n_samples)\n  \n  Returns:\n    ndarray: estimated parameter values of shape (n_regressors)\n  \"\"\"\n  \n  # Compute theta_hat using OLS\n  theta_hat = np.linalg.inv(X.T@X)@X.T@y\n\n  return theta_hat\n\n\n# Uncomment below to test your function\ntheta_hat = ordinary_least_squares(X, y)\ny_hat = X @ theta_hat\n\nprint(f'intercept = {theta_hat[0]:.2f}, customer satisfaction = {theta_hat[1]:.2f}, price = {theta_hat[2]:.2f}')\nprint(f\"MSE = {np.mean((y - y_hat)**2):.2f}\")\n\nintercept = 2.72, customer satisfaction = 2.94, price = -3.07\nMSE = 0.91"
  }
]